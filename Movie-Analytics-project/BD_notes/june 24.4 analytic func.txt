from pyspark.sql import *
from pyspark.sql.functions import *

spark = SparkSession.builder.master("local").appName("test").getOrCreate()
data=r"D:\bigdata\drivers\us-500.csv"
df=spark.read.format("csv").option("header","true").option("inferSchema","true").load(data)
df=df.withColumnRenamed("zip","sal").withColumnRenamed("first_name","fname")

win=Window.partitionBy("state").orderBy(col("sal").desc())
ndf=df.withColumn("ld",lead(col("sal")).over(win))\
    .withColumn("lg",lag(col("sal")).over(win)).na.fill(0)\
    .withColumn("fst",first(col("sal")).over(win))
ndf.show(500)
#withColumnRenamed used to rename only one column at a time.
#drop(col) ..let eg: 1000 columns ... if u select 980 cols .. select(col1, col2, col3, col4
#if u use drop (col,col2,col3) .... u have ll columns but pls ignore these specified 3 columns ..