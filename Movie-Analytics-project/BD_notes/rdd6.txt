from pyspark.sql import *
from pyspark.sql.functions import *

spark = SparkSession.builder.master("local[2]").appName("test").getOrCreate()
sc=spark.sparkContext
data=r"E:\bigdata\drivers\donations.csv"
ardd = sc.textFile(data)

proc = ardd.filter(lambda x:"dt" not in x).map(lambda x:x.split(",")).map(lambda x:(x[0],int(x[2])))\
    .reduceByKey(lambda x,y:x+y)

#if u want to group by must have key, value format
for x in proc.take(9):
    print(x)